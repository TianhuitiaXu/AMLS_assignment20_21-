{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#-------------read data_panda\n",
    "name = ['gender', 'smiling']\n",
    "#data = pd.read_csv('./dataset_AMLS_20-21/celeba/labels.csv',sep='\\t',header=0,names = name)\n",
    "data = pd.read_csv('./dataset_AMLS_20-21/celeba/labels.csv',sep='\\t')\n",
    "Y_A10=data['gender']\n",
    "Y_A11=data['smiling']\n",
    "y_1 = Y_A10\n",
    "y_2 = Y_A11\n",
    "for i in range(y_1.shape[0]):\n",
    "    if y_1[i] == -1:\n",
    "        y_1[i] = 0\n",
    "    else:\n",
    "        y_1[i] = 1 #防止标签出现负数，如果这里负数的话后面计算loss就会出现nan\n",
    "for i in range(y_2.shape[0]):\n",
    "    if y_2[i] == -1:\n",
    "        y_2[i] = 0\n",
    "    else:\n",
    "        y_2[i] = 1\n",
    "\n",
    "data_csv = open('data.csv', 'r', newline = '')\n",
    "#read cvs file\n",
    "reader = csv.reader(data_csv)\n",
    "rows= [row for row in reader]\n",
    "data_csv.close()\n",
    "X = np.zeros((5000,38804),dtype='float')\n",
    "for i in range(len(rows)):\n",
    "    str0 = listToString(rows[i])\n",
    "    x = str0.split(' ')\n",
    "#list-str-float\n",
    "    for j in range(X.shape[1]):\n",
    "        x[j] = float(x[j])\n",
    "    X[i,:] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import dlib\n",
    "import pandas as pd\n",
    "#import defusedxml\n",
    "\n",
    "# PATH TO ALL IMAGES\n",
    "global basedir, image_paths, target_size\n",
    "#basedir = './AMLS_assignment20_21-/dataset_AMLS_20-21'\n",
    "basedir = './dataset_AMLS_20-21'\n",
    "basedir = os.path.join(basedir,'celeba')\n",
    "#images_dir = basedir\n",
    "images_dir = os.path.join(basedir,'img')\n",
    "labels_filename = 'labels.csv'\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((shape.num_parts, 2), dtype=dtype)\n",
    "\n",
    "    # loop over all facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, shape.num_parts):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords\n",
    "\n",
    "def rect_to_bb(rect):\n",
    "    # take a bounding predicted by dlib and convert it\n",
    "    # to the format (x, y, w, h) as we would normally do\n",
    "    # with OpenCV\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "\n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)\n",
    "\n",
    "\n",
    "def run_dlib_shape(image):\n",
    "    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks\n",
    "    # load the input image, resize it, and convert it to grayscale\n",
    "    resized_image = image.astype('uint8')\n",
    "\n",
    "    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = gray.astype('uint8')\n",
    "\n",
    "    # detect faces in the grayscale image\n",
    "    rects = detector(gray, 1)\n",
    "    num_faces = len(rects)\n",
    "\n",
    "    if num_faces == 0:\n",
    "        return None, resized_image\n",
    "\n",
    "    face_areas = np.zeros((1, num_faces))\n",
    "    face_shapes = np.zeros((136, num_faces), dtype=np.int64)\n",
    "\n",
    "    # loop over the face detections\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        temp_shape = predictor(gray, rect)\n",
    "        temp_shape = shape_to_np(temp_shape)\n",
    "\n",
    "        # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "        # [i.e., (x, y, w, h)],\n",
    "        #   (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        (x, y, w, h) = rect_to_bb(rect)\n",
    "        face_shapes[:, i] = np.reshape(temp_shape, [136])\n",
    "        face_areas[0, i] = w * h\n",
    "    # find largest face and keep\n",
    "    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])\n",
    "\n",
    "    return dlibout, resized_image\n",
    "\n",
    "# def listToString(s):\n",
    "#     str1 = \" \"\n",
    "#     return (str1.join(s))\n",
    "\n",
    "def extract_features_labels():\n",
    "    \"\"\"\n",
    "    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.\n",
    "    It also extracts the gender label for each image.\n",
    "    :return:\n",
    "        landmark_features:  an array containing 68 landmark points for each image in which a face was detected\n",
    "        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in\n",
    "                            which a face was detected\n",
    "    \"\"\"\n",
    "    print(images_dir)\n",
    "    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]\n",
    "    target_size = None\n",
    "    \n",
    "    name = ['gender', 'smiling']\n",
    "    #data = pd.read_csv('./dataset_AMLS_20-21/celeba/labels.csv',sep='\\t',header=0,names = name)\n",
    "    data = pd.read_csv(os.path.join(basedir, labels_filename),sep='\\t')\n",
    "#     Y_A10=data['gender']\n",
    "#     Y_A11=data['smiling']\n",
    "#     y_1 = Y_A10\n",
    "#     y_2 = Y_A11\n",
    "    gender_labels = data['gender']\n",
    "    smiling = data['smiling']\n",
    "    #print(gender_labels[0:5])\n",
    "\n",
    "#     labels_file = open(os.path.join(basedir, labels_filename), 'r')\n",
    "#     lines = labels_file.readlines() \n",
    "# # from list get str get array symbols\n",
    "# for i in range(len(rows)):\n",
    "#     str0 = listToString(rows[i])\n",
    "#     y = str0.split('\\t')\n",
    "#     y = np.array(y)\n",
    "#     Y_A1[i,0] = y[2]\n",
    "#     Y_A1[i,1] = y[3]\n",
    "#     #str = str+str0\n",
    "    #gender_labels = {listToString(line).split('\\t')[0] : int(listToString(line).split('\\t')[3]) for line in lines[1:]}\n",
    "    #gender_labels = {listToString(line).split('\\t')[3] for line in lines[1:]}\n",
    "    \n",
    "#     for line in lines[1:10]:\n",
    "#         #print(line)\n",
    "#         gender_labels.append(listToString(line).split('\\t')[2])\n",
    "#         a = listToString(line).split('\\t')\n",
    "#         print(np.array(a)[2])\n",
    "#     #gender_labels = np.array(gender_labels)\n",
    "                      \n",
    "    if os.path.isdir(images_dir):\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        for img_path in image_paths:\n",
    "            #print(img_path)\n",
    "            file_name= img_path.split('.')[1].split('\\\\')[-1]\n",
    "            file_name=int(file_name)\n",
    "            #print(file_name)\n",
    "            # load image\n",
    "            img = image.img_to_array(\n",
    "                image.load_img(img_path,\n",
    "                               target_size=target_size,\n",
    "                               interpolation='bicubic'))\n",
    "            #print(img)\n",
    "            features, _ = run_dlib_shape(img)\n",
    "            #print(features)\n",
    "            if features is not None:\n",
    "                all_features.append(features)\n",
    "                #print(gender_labels[file_name])\n",
    "                all_labels.append(smiling[file_name])\n",
    "                #all_labels.append(gender_labels[file_name])\n",
    "    #print(all_labels)\n",
    "\n",
    "    landmark_features = np.array(all_features)\n",
    "    gender_labels = (np.array(all_labels, dtype=float) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1\n",
    "    return landmark_features, gender_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 58, 113],\n",
       "         [ 57, 123],\n",
       "         [ 57, 133],\n",
       "         ...,\n",
       "         [ 88, 164],\n",
       "         [ 83, 163],\n",
       "         [ 79, 162]],\n",
       " \n",
       "        [[ 55, 111],\n",
       "         [ 53, 121],\n",
       "         [ 54, 132],\n",
       "         ...,\n",
       "         [ 89, 162],\n",
       "         [ 83, 162],\n",
       "         [ 79, 161]],\n",
       " \n",
       "        [[ 44, 102],\n",
       "         [ 45, 115],\n",
       "         [ 47, 127],\n",
       "         ...,\n",
       "         [ 94, 160],\n",
       "         [ 89, 161],\n",
       "         [ 84, 161]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 49, 116],\n",
       "         [ 50, 127],\n",
       "         [ 52, 138],\n",
       "         ...,\n",
       "         [ 91, 158],\n",
       "         [ 87, 159],\n",
       "         [ 84, 158]],\n",
       " \n",
       "        [[ 60, 117],\n",
       "         [ 60, 126],\n",
       "         [ 60, 136],\n",
       "         ...,\n",
       "         [ 89, 153],\n",
       "         [ 84, 153],\n",
       "         [ 80, 153]],\n",
       " \n",
       "        [[ 53, 112],\n",
       "         [ 53, 122],\n",
       "         [ 53, 132],\n",
       "         ...,\n",
       "         [ 91, 158],\n",
       "         [ 86, 158],\n",
       "         [ 81, 157]]], dtype=int64),\n",
       " array([0., 0., 0., ..., 0., 1., 0.]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lab2_landmarks as l2\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset_AMLS_20-21\\celeba\\img\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-cf790a06f3b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-123-9079853cf7a5>\u001b[0m in \u001b[0;36mextract_features_labels\u001b[1;34m()\u001b[0m\n\u001b[0;32m    140\u001b[0m                 image.load_img(img_path,\n\u001b[0;32m    141\u001b[0m                                \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                                interpolation='bicubic'))\n\u001b[0m\u001b[0;32m    143\u001b[0m             \u001b[1;31m#print(img)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_dlib_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mytf\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[0;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'grayscale'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'L'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mytf\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2890\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2891\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2892\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, y = extract_features_labels()\n",
    "print(X.shape(),y.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "\n",
    "    X, y = extract_features_labels()\n",
    "    Y = np.array([y, -(y - 1)]).T\n",
    "    # Shuffle and split the data into training and test set\n",
    "    X, Y = shuffle(X,Y)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)\n",
    "#     tr_X = X[:100]\n",
    "#     tr_Y = Y[:100]\n",
    "#     te_X = X[100:]\n",
    "#     te_Y = Y[100:]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "#    return tr_X, tr_Y, te_X, te_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset_AMLS_20-21\\celeba\\img\n",
      "(3356, 68, 2) (1439, 68, 2)\n"
     ]
    }
   ],
   "source": [
    "# sklearn functions implementation\n",
    "def img_SVM(training_images, training_labels, test_images, test_labels):\n",
    "    classifier = SVC(kernel='linear')\n",
    "    classifier.fit(training_images, training_labels)\n",
    "    pred = classifier.predict(test_images)\n",
    "    print(\"Accuracy:\", accuracy_score(test_labels, pred))\n",
    "\n",
    "   # print(pred)\n",
    "    return pred\n",
    "\n",
    "tr_X, tr_Y, te_X, te_Y= get_data()\n",
    "print(tr_X.shape, te_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.881862404447533\n",
      "[0. 0. 1. ... 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "pred=img_SVM(tr_X.reshape((3356, 68*2)), list(zip(*tr_Y))[0], te_X.reshape((1439, 68*2)), list(zip(*te_Y))[0])\n",
    "print(pred)\n",
    "#小结：提取图片特征后再应用SVM，得到gender准确率为0.9200833912439194；得到smiling准确率为0.881862404447533；引用了什么库？Keras.preprocessing;dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
