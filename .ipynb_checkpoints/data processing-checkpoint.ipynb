{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "#import os\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Flatten \n",
    "from keras.layers import Conv2D, MaxPooling2D \n",
    "from keras.utils import to_categorical \n",
    "from keras.preprocessing import image \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
    "#from keras.utils import to_categorical \n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 4)\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "# import numpy as np\n",
    "# #-----------read data_csv.reader\n",
    "# #csv_file = open('/Users/93554/AMLS_assignment20_21-/dataset_AMLS_20-21/celeba/labels.csv', 'r', newline = '', encoding = 'utf-8')\n",
    "# csv_file = open('./dataset_AMLS_20-21/celeba/labels.csv', 'r', newline = '')\n",
    "# #read cvs file\n",
    "# reader = csv.reader(csv_file)\n",
    "# rows= [row for row in reader]\n",
    "# csv_file.close()\n",
    "# #create symbols(gender,smiling)\n",
    "# Y_A1 = np.zeros((5000,2),dtype='int64')\n",
    "# #neglet the first row of names\n",
    "# rows = rows[1:]\n",
    "# # Function to convert   \n",
    "# def listToString(s):      \n",
    "#     str1 = \" \"      \n",
    "#     return (str1.join(s)) \n",
    "# # from list get str get array symbols\n",
    "# for i in range(len(rows)):\n",
    "#     str0 = listToString(rows[i])\n",
    "#     y = str0.split('\\t')\n",
    "#     y = np.array(y)\n",
    "#     Y_A1[i,0] = y[2]\n",
    "#     Y_A1[i,1] = y[3]\n",
    "#     #str = str+str0\n",
    "#-------------read data_panda\n",
    "name = ['gender', 'smiling']\n",
    "#data = pd.read_csv('./dataset_AMLS_20-21/celeba/labels.csv',sep='\\t',header=0,names = name)\n",
    "data = pd.read_csv('./dataset_AMLS_20-21/celeba/labels.csv',sep='\\t')\n",
    "Y_A10=data['gender']\n",
    "Y_A11=data['smiling']\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]C:\\Users\\93554\\anaconda3\\envs\\mytf\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:104: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n",
      "100%|██████████| 5000/5000 [00:04<00:00, 1055.59it/s]\n"
     ]
    }
   ],
   "source": [
    "train_image = [] \n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    #img = image.load_img('./dataset_AMLS_20-21/celeba/img/'+str(i)+'.jpg', color_mode='rgb', target_size=None, grayscale=False)\n",
    "    img = image.load_img('./dataset_AMLS_20-21/celeba/img/'+str(i)+'.jpg', target_size=None, grayscale=True)\n",
    "    img = image.img_to_array(img)     \n",
    "    img = img/255     \n",
    "    train_image.append(img) \n",
    "X = np.array(train_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(5000, 218, 178, 1)\n",
      "(5000, 38804)\n",
      "0.9137255\n"
     ]
    }
   ],
   "source": [
    "#debug\n",
    "print(type(train_image))\n",
    "print(X.shape)\n",
    "nsamples, nx, ny, nz = X.shape\n",
    "X_A = X.reshape((nsamples,nx*ny*nz))\n",
    "print(XX.shape)\n",
    "print(XX[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把存下来的X读出来就不用每次训练数据预处理了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open('Data.csv','w',newline='',encoding='utf-8')\n",
    "writer = csv.writer(csv_file)\n",
    "for i in range(XX.shape[0]):\n",
    "    writer.writerow(XX[i,:])\n",
    "csv_file.close() \n",
    "#hmmm...存的时候多加了np.array结果好像多了个【】不好读取了..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#panda文件读写\n",
    "import pandas as pd\n",
    "#写\n",
    "#任意的多组列表\n",
    "a = [1,2,3]\n",
    "b = [4,5,6]    \n",
    "\n",
    "#字典中的key值即为csv中列名\n",
    "dataframe = pd.DataFrame({'a_name':a,'b_name':b})\n",
    "\n",
    "#将DataFrame存储为csv,index表示是否显示行名，default=True\n",
    "dataframe.to_csv(\"test.csv\",index=False,sep=',')\n",
    "\n",
    "#读\n",
    "data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-974272fb3a02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata_csv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mXX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m38804\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mstr0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlistToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "data_csv = open('data.csv', 'r', newline = '')\n",
    "#read cvs file\n",
    "reader = csv.reader(data_csv)\n",
    "rows= [row for row in reader]\n",
    "data_csv.close()\n",
    "XX = np.zeros((5000,38804),dtype='float')\n",
    "for i in range(len(rows)):\n",
    "    str0 = listToString(rows[i])\n",
    "    x = str0.split(' ')\n",
    "    #x = np.array(x)\n",
    "#list-str-float\n",
    "#     for j in range(XX.shape[1]):\n",
    "#         x[j] = float(x[j])\n",
    "#     XX[i,:] = x\n",
    "print(x)\n",
    "print(type(x[0]))\n",
    "float(x[0])\n",
    "# print(Y)\n",
    "# print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_A, Y_A10,random_state=0)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.89      0.87      0.88       635\n",
      "           1       0.87      0.89      0.88       615\n",
      "\n",
      "    accuracy                           0.88      1250\n",
      "   macro avg       0.88      0.88      0.88      1250\n",
      "weighted avg       0.88      0.88      0.88      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sklearn functions implementation\n",
    "#gender:'sagal2'0.8664fail;'sagal1'0.8776fail;'liblinearl1'0.88succes;\n",
    "#smile:'liblinear'0.8712;'sagal1'0.879fail;\n",
    "def logRegrPredict(x_train, y_train, x_test):\n",
    "    # Build Logistic Regression Model\n",
    "    logreg = LogisticRegression(penalty='l1',solver='liblinear',random_state=0)\n",
    "    # Train the model using the training sets\n",
    "    logreg.fit(x_train, y_train)\n",
    "    y_pred= logreg.predict(x_test)\n",
    "    #print('Accuracy on test set: {:.2f}'.format(logreg.score(x_test, y_test)))\n",
    "    return y_pred\n",
    "\n",
    "y_pred = logRegrPredict(x_train, y_train, x_test)\n",
    "#print(y_pred)\n",
    "print('Accuracy on test set: '+str(accuracy_score(y_test,y_pred)))\n",
    "print(classification_report(y_test,y_pred))#text report showing the main classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.5192\n"
     ]
    }
   ],
   "source": [
    "#'rbf'0.8416\n",
    "from sklearn.svm import NuSVC\n",
    "clf = NuSVC(kernel='rbf')\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print('Accuracy on test set: '+str(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_6 to have 2 dimensions, but got array with shape (3750, 2, 2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-2153c7cdbe2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test),report_tensor_allocations_upon_oom)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m#report_tensor_allocations_upon_oom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#prediction = model.predict_classes(test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mytf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mytf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mytf\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_6 to have 2 dimensions, but got array with shape (3750, 2, 2, 2)"
     ]
    }
   ],
   "source": [
    "# Epoch 1/10\n",
    "#  352/3750 [=>............................] - ETA: 42s - loss: 0.0615 - accuracy: 0.9659\n",
    "# gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "# session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "YY1 = to_categorical(YY1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, YY1,random_state=0)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(218,178,1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "#model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test),report_tensor_allocations_upon_oom)\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "#report_tensor_allocations_upon_oom\n",
    "#prediction = model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
